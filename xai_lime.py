# -*- coding: utf-8 -*-
"""XAI_LIME.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1laIpyJcncMOlc_gPmr58P1KeTj_jUXQH
"""

!pip install lime

import pandas as pd
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import lime
import lime.lime_tabular
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from keras.models import load_model
from sklearn.pipeline import Pipeline

from skimage.segmentation import mark_boundaries
from sklearn.ensemble import RandomForestClassifier
from lime import lime_tabular, lime_text, lime_image
import matplotlib.pyplot as plt

# Load data
result = pd.read_csv('Data_Resampled_Desktop_PCA10.csv')

# Separate input features and target variable
X = result.iloc[:, :-1].values
y = result.iloc[:, -1].values

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create random forest model
model = RandomForestClassifier()
model.fit(X_train, y_train)

model2 = LogisticRegression()
model2.fit(X_train, y_train)

# Create explainer
feature_names = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9', 'feature10']
explainer_lime = lime_tabular.LimeTabularExplainer(X_train,
                                                   feature_names=feature_names,
                                                   verbose=True,
                                                   mode='regression')
#explainer = lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=['0', '1'], discretize_continuous=False)


# Index corresponding to the test vector
i = 10

# Number denoting the top features
k = 5

# Calling the explain_instance method by passing in the:
#    1) ith test vector
#    2) prediction function used by our prediction model('reg' in this case)
#    3) the top features which we want to see, denoted by k

exp_lime = explainer_lime.explain_instance(X_test[i], model.predict, num_features=k)

# Finally visualizing the explanations
exp_lime.show_in_notebook()


explainer_lime = lime_tabular.LimeTabularExplainer(X_train,
                                                   feature_names=feature_names,
                                                   verbose=True,
                                                   mode='regression')
#explainer = lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=['0', '1'], discretize_continuous=False)


# Index corresponding to the test vector
i = 10

# Number denoting the top features
k = 5

# Calling the explain_instance method by passing in the:
#    1) ith test vector
#    2) prediction function used by our prediction model('reg' in this case)
#    3) the top features which we want to see, denoted by k

exp_lime = explainer_lime.explain_instance(
    X_test[i], model2.predict, num_features=k)

# Finally visualizing the explanations
exp_lime.show_in_notebook()

# Generate explanation for instance
#instance = X_test[0]
#exp = explainer.explain_instance(instance, model.predict_proba, num_features=len(feature_names))
#exp.show_in_notebook()


"""
# Load data
data = pd.read_csv('data_resampled_2.csv')

X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values

# Standardize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

n_features = X.shape[1]
X = X.reshape((X.shape[0], 1, n_features))

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create LSTM model
model = Sequential()
model.add(LSTM(50, input_shape=(1, n_features)))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Fit model to training data
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# Create explainer
feature_names = ['pca_1', 'pca_2']
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.reshape(X_train.shape[0], -1),
                                                   feature_names=feature_names,
                                                   class_names=['0', '1'])

# Generate explanation for instance
instance = X_test[0]
exp = explainer.explain_instance(instance.reshape(1, -1),
                                  model.predict_proba,
                                  num_features=len(feature_names))

# Print explanation
print('Explanation for class:', exp.available_labels()[0])
print(exp.as_list())

model = LogisticRegression()
model.fit(X_train, y_train)

feature_names = ['pca_1', 'pca_2']

explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=['0', '1'])
instance = X_test[0]

# Generate an explanation for the instance using LIME
exp = explainer.explain_instance(instance, model.predict_proba, num_features=len(feature_names))

# Print the explanation
print('Explanation for class:', exp.available_labels()[0])
print(exp.as_list())"""