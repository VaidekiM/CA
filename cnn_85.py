# -*- coding: utf-8 -*-
"""CNN_85.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19OBsEtTp-ikf_3fHxdmHS2Rrmzwb_Y6E
"""

import pandas as pd
import numpy as np
import numpy as py
from keras.models import Sequential
from keras.layers import Dense, LSTM
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
from keras.metrics import Accuracy
from keras.layers import Dropout
from sklearn.decomposition import PCA
from keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
from keras.metrics import Accuracy
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, confusion_matrix


# Load data from CSV file
data = pd.read_csv('data_resampled.csv')
x = data.iloc[:, : -1]
y= data.iloc[:, -1]

# Define the CNN model
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(units=128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
early_stopping = EarlyStopping(monitor='val_loss', patience=5)
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])

model.summary()

# Evaluate the model on the test set
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

if conf_matrix.shape[1] == 2:
    far = conf_matrix[0, 1] / (conf_matrix[0, 0] + conf_matrix[0, 1]) if conf_matrix[0, 0] + conf_matrix[0, 1] != 0 else 0  # False Acceptance Rate (FAR)
else:
    far = 0

frr = conf_matrix[1, 0] / (conf_matrix[1, 0] + conf_matrix[1, 1])  # False Rejection Rate (FRR)

print('F1 score:', f1)
print('FAR:', far)
print('FRR:', frr)

loss, accuracy = model.evaluate(X_test, y_test)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training and validation loss per epoch
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
# Evaluate the model on the test set

#loss, accuracy = model.evaluate(X_test, y_test)
#print('Test loss:', loss)
#print('Test accuracy:', accuracy)